{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/others10/blob/master/clip_interrogator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytxkysgmrJEi"
      },
      "source": [
        "# CLIP Interrogator by [@pharmapsychotic](https://twitter.com/pharmapsychotic) \n",
        "\n",
        "<br>\n",
        "\n",
        "What do the different OpenAI CLIP models see in an image? What might be a good text prompt to create similar images using CLIP guided diffusion or another text to image model? The CLIP Interrogator is here to get you answers!\n",
        "\n",
        "<br>\n",
        "\n",
        "If this notebook is helpful to you please consider buying me a coffee via [ko-fi](https://ko-fi.com/pharmapsychotic) or following me on [twitter](https://twitter.com/pharmapsychotic) for more cool Ai stuff. ðŸ™‚\n",
        "\n",
        "And if you're looking for more Ai art tools check out my [Ai generative art tools list](https://pharmapsychotic.com/tools.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YQk0eemUrSC7"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "30xPxDSDrJEl"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "!pip3 install ftfy regex tqdm transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "!pip3 install git+https://github.com/openai/CLIP.git\n",
        "!git clone -b v1 https://github.com/pharmapsychotic/clip-interrogator.git\n",
        "!git clone https://github.com/salesforce/BLIP\n",
        "%cd /content/BLIP\n",
        "\n",
        "import clip\n",
        "import gc\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from models.blip import blip_decoder\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "blip_image_eval_size = 384\n",
        "blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'        \n",
        "blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='base')\n",
        "blip_model.eval()\n",
        "blip_model = blip_model.to(device)\n",
        "\n",
        "def generate_caption(pil_image):\n",
        "    gpu_image = transforms.Compose([\n",
        "        transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    ])(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
        "    return caption[0]\n",
        "\n",
        "def load_list(filename):\n",
        "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items\n",
        "\n",
        "def rank(model, image_features, text_array, top_count=1):\n",
        "    top_count = min(top_count, len(text_array))\n",
        "    text_tokens = clip.tokenize([text for text in text_array]).cuda()\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    similarity = torch.zeros((1, len(text_array))).to(device)\n",
        "    for i in range(image_features.shape[0]):\n",
        "        similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
        "    similarity /= image_features.shape[0]\n",
        "\n",
        "    top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)  \n",
        "    return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n",
        "\n",
        "def interrogate(image, models):\n",
        "    caption = generate_caption(image)\n",
        "    if len(models) == 0:\n",
        "        print(f\"\\n\\n{caption}\")\n",
        "        return\n",
        "\n",
        "    table = []\n",
        "    bests = [[('',0)]]*5\n",
        "    for model_name in models:\n",
        "        print(f\"Interrogating with {model_name}...\")\n",
        "        model, preprocess = clip.load(model_name)\n",
        "        model.cuda().eval()\n",
        "\n",
        "        images = preprocess(image).unsqueeze(0).cuda()\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(images).float()\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        ranks = [\n",
        "            rank(model, image_features, mediums),\n",
        "            rank(model, image_features, [\"by \"+artist for artist in artists]),\n",
        "            rank(model, image_features, trending_list),\n",
        "            rank(model, image_features, movements),\n",
        "            rank(model, image_features, flavors, top_count=3)\n",
        "        ]\n",
        "\n",
        "        for i in range(len(ranks)):\n",
        "            confidence_sum = 0\n",
        "            for ci in range(len(ranks[i])):\n",
        "                confidence_sum += ranks[i][ci][1]\n",
        "            if confidence_sum > sum(bests[i][t][1] for t in range(len(bests[i]))):\n",
        "                bests[i] = ranks[i]\n",
        "\n",
        "        row = [model_name]\n",
        "        for r in ranks:\n",
        "            row.append(', '.join([f\"{x[0]} ({x[1]:0.1f}%)\" for x in r]))\n",
        "\n",
        "        table.append(row)\n",
        "\n",
        "        del model\n",
        "        gc.collect()\n",
        "    display(pd.DataFrame(table, columns=[\"Model\", \"Medium\", \"Artist\", \"Trending\", \"Movement\", \"Flavors\"]))\n",
        "\n",
        "    flaves = ', '.join([f\"{x[0]}\" for x in bests[4]])\n",
        "    medium = bests[0][0][0]\n",
        "    if caption.startswith(medium):\n",
        "        print(f\"\\n\\n{caption} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\")\n",
        "    else:\n",
        "        print(f\"\\n\\n{caption}, {medium} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\")\n",
        "\n",
        "data_path = \"../clip-interrogator/data/\"\n",
        "\n",
        "artists = load_list(os.path.join(data_path, 'artists.txt'))\n",
        "flavors = load_list(os.path.join(data_path, 'flavors.txt'))\n",
        "mediums = load_list(os.path.join(data_path, 'mediums.txt'))\n",
        "movements = load_list(os.path.join(data_path, 'movements.txt'))\n",
        "\n",
        "sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "trending_list = [site for site in sites]\n",
        "trending_list.extend([\"trending on \"+site for site in sites])\n",
        "trending_list.extend([\"featured on \"+site for site in sites])\n",
        "trending_list.extend([site+\" contest winner\" for site in sites])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rbDEMDGJrJEo"
      },
      "outputs": [],
      "source": [
        "#@title Interrogate!\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown #####**Image:**\n",
        "\n",
        "image_path_or_url = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown #####**CLIP models:**\n",
        "\n",
        "#@markdown For [StableDiffusion](https://stability.ai/blog/stable-diffusion-announcement) you can just use ViTL14<br>\n",
        "#@markdown For [DiscoDiffusion](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb) and \n",
        "#@markdown [JAX](https://colab.research.google.com/github/huemin-art/jax-guided-diffusion/blob/v2.7/Huemin_Jax_Diffusion_2_7.ipynb) enable all the same models here as you intend to use when generating your images\n",
        "\n",
        "ViTB32 = False #@param{type:\"boolean\"}\n",
        "ViTB16 = False #@param{type:\"boolean\"}\n",
        "ViTL14 = True #@param{type:\"boolean\"}\n",
        "ViTL14_336px = False #@param{type:\"boolean\"}\n",
        "RN101 = False #@param{type:\"boolean\"}\n",
        "RN50 = False #@param{type:\"boolean\"}\n",
        "RN50x4 = False #@param{type:\"boolean\"}\n",
        "RN50x16 = False #@param{type:\"boolean\"}\n",
        "RN50x64 = False #@param{type:\"boolean\"}\n",
        "\n",
        "models = []\n",
        "if ViTB32: models.append('ViT-B/32')\n",
        "if ViTB16: models.append('ViT-B/16')\n",
        "if ViTL14: models.append('ViT-L/14')\n",
        "if ViTL14_336px: models.append('ViT-L/14@336px')\n",
        "if RN101: models.append('RN101')\n",
        "if RN50: models.append('RN50')\n",
        "if RN50x4: models.append('RN50x4')\n",
        "if RN50x16: models.append('RN50x16')\n",
        "if RN50x64: models.append('RN50x64')\n",
        "\n",
        "if str(image_path_or_url).startswith('http://') or str(image_path_or_url).startswith('https://'):\n",
        "    image = Image.open(requests.get(image_path_or_url, stream=True).raw).convert('RGB')\n",
        "else:\n",
        "    image = Image.open(image_path_or_url).convert('RGB')\n",
        "\n",
        "thumb = image.copy()\n",
        "thumb.thumbnail([blip_image_eval_size, blip_image_eval_size])\n",
        "display(thumb)\n",
        "\n",
        "interrogate(image, models=models)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d4dd9c310c32a31bb53615812f2f2c6cba010b7aa4dfb14e2b192e650667fecd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}